{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd,gluon,init,autograd, metric\n",
    "\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import datasets,transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape : (28, 28, 1) and Datatype : <class 'numpy.uint8'>\n",
      "Number of Intiger :  60000\n"
     ]
    }
   ],
   "source": [
    "mainst_train = datasets.FashionMNIST(train=True)\n",
    "X,y = mainst_train[0]\n",
    "\n",
    "print(\"Shape : {} and Datatype : {}\".format(X.shape,X.dtype))\n",
    "print(\"Number of Intiger : \",len(mainst_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_idx = 123\n",
    "# sample_data, sample_label = mainst_train[sample_idx]\n",
    "# plt.imshow(sample_data.asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([transforms.ToTensor(),transforms.Normalize(0.13,0.31)])\n",
    "\n",
    "mainst_train = mainst_train.transform_first(transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_data = gluon.data.DataLoader(mainst_train,batch_size=batch_size,shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 1, 28, 28) (256,)\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_data:\n",
    "    print(data.shape,label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(\n",
    "        nn.Conv2D(channels=6,kernel_size=5,activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=2,strides=2),\n",
    "        nn.Conv2D(channels=16,kernel_size=3,activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=2,strides=2),\n",
    "        nn.Flatten(),\n",
    "        nn.Dense(120,activation='relu'),\n",
    "        nn.Dense(84,activation='relu'),\n",
    "        nn.Dense(10)\n",
    "    )\n",
    "    \n",
    "net.initialize(init=init.Xavier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = metric.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss: 0.836 Acc: 0.692 Pref: 3590.7 img/sec\n",
      "Epoch: 1 loss: 0.470 Acc: 0.759 Pref: 3595.8 img/sec\n",
      "Epoch: 2 loss: 0.399 Acc: 0.790 Pref: 3581.8 img/sec\n",
      "Epoch: 3 loss: 0.361 Acc: 0.810 Pref: 3603.0 img/sec\n",
      "Epoch: 4 loss: 0.334 Acc: 0.823 Pref: 3511.5 img/sec\n",
      "Epoch: 5 loss: 0.315 Acc: 0.834 Pref: 3594.6 img/sec\n",
      "Epoch: 6 loss: 0.301 Acc: 0.841 Pref: 3573.4 img/sec\n",
      "Epoch: 7 loss: 0.289 Acc: 0.848 Pref: 3612.3 img/sec\n",
      "Epoch: 8 loss: 0.280 Acc: 0.853 Pref: 3598.7 img/sec\n",
      "Epoch: 9 loss: 0.270 Acc: 0.858 Pref: 3569.8 img/sec\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    train_loss = 0.\n",
    "    tic = time()\n",
    "    \n",
    "    for data, label in train_data:\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output,label)\n",
    "        loss.backward()\n",
    "        \n",
    "        trainer.step(batch_size)\n",
    "        \n",
    "        train_loss += loss.mean().asscalar()\n",
    "        train_acc.update(label,output)\n",
    "        \n",
    "    print(\"Epoch: %d loss: %.3f Acc: %.3f Pref: %.1f img/sec\"%(epoch,train_loss/len(train_data),train_acc.get()[1],len(mainst_train)/(time()-tic)))\n",
    "\n",
    "net.save_parameters('trained_params.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
